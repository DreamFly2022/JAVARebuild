# 进阶实战：SQL和性能优化

[toc]

## 一、回顾印象最深刻的点

1. 配置优化：query_cache_size、tmp_table_size

   [参数调优建议](https://help.aliyun.com/document_detail/63255.html?utm_content=g_1000230851&spm=5176.20966629.toubu.3.f2991ddcpxxvD1#title-zwl-lne-jae)

   ```sql
   适用版本：5.7、5.6、5.5
   默认值：3145728
   修改完后是否需要重启：否
   作用：该参数用于控制MySQL query cache的内存大小。如果MySQL开启query cache，在执行每一个query的时候会先锁住query cache，然后判断是否存在于query cache中，如果存在则直接返回结果，如果不存在，则再进行引擎查询等操作。同时，insert、update和delete这样的操作都会将query cahce失效掉，这种失效还包括结构或者索引的任何变化。但是cache失效的维护代价较高，会给MySQL带来较大的压力。所以，当数据库不会频繁更新时，query cache是很有用的，但如果写入操作非常频繁并集中在某几张表上，那么query cache lock的锁机制就会造成很频繁的锁冲突，对于这一张表的写和读会互相等待query cache lock解锁，从而导致select的查询效率下降。
   现象：数据库中有大量的连接状态为checking query cache for query、Waiting for query cache lock、storing result in query cache。
   修改建议：(阿里云数据库 RDS)默认是关闭query cache功能的，如果您的实例打开了query cache，当出现上述情况后可以关闭query cache。
   ```

   ```
   tmp_table_size
   适用版本：8.0、5.7、5.6、5.5
   默认值：2097152
   修改完后是否需要重启：否
   作用：该参数用于决定内部内存临时表的最大值，每个线程都要分配，实际起限制作用的是tmp_table_size和max_heap_table_size的最小值。如果内存临时表超出了限制，MySQL就会自动地把它转化为基于磁盘的MyISAM表。优化查询语句的时候，要避免使用临时表，如果实在避免不了的话，要保证这些临时表是存在内存中的。
   现象：如果复杂的SQL语句中包含了group by、distinct等不能通过索引进行优化而使用了临时表，则会导致SQL执行时间加长。
   修改建议：如果应用中有很多group by、distinct等语句，同时数据库有足够的内存，可以增大tmp_table_size（max_heap_table_size）的值，以此来提升查询性能。
   ```

2. 如何设计表：char和varchar；

3. 索引的存储：

   - hash索引和B+tree
   - 索引失效

## 二、进阶实践

### 2.1 思考在实际项目中的应用

1、（选做）：基于课程中的设计原则和最佳实践，分析是否可以将自己负责的业务系统 进行数据库设计或是数据库服务器方面的优化

（1）配置优化：

- 对于主要是查询操作，修改操作（insert、update、delete）少的情况下，可以开启查询缓存；
- 适当修改tmp_table_size，但不让其超过内存大小；

（2）表结构

- 没有特殊情况，就是用InnoDB引擎；

- 选择合适的字段类型：

  - 单机下，尽量使用int、bigint作为主键（自增）；

  - 对于char和varchar，长度一定的时候，使用char；使用varchar时候，尽量让其长度不超过255;

    > varchar 超过255，比如256，就需要2个字节作为前缀索引。

  - 为了存储更精确的float类型，可以将值乘以10的倍数，当作int存入，计算的时候再除以10的倍数；

- 索引的使用：

  - 避免索引失效；
  - 选择区分度高的字段作为索引；

- 驱动表

  尽量用小表驱动大表

### 2.2 设计一套简单的表结构

2、（必做）：基于电商交易场景（用户、商品、订单），设计一套简单的表结构，提交 DDL 的 SQL 文件到 Github（后面2周的作业依然要是用到这个表结构）

```mysql
CREATE TABLE ds_shop.`t_user` (
  `u_id`         bigint(20)   NOT NULL PRIMARY KEY AUTO_INCREMENT  COMMENT '主键，自增',
  `username`     varchar(255) NOT NULL COMMENT '用户名',
  `phone_number` char(11)     NOT NULL COMMENT '电话号码',
  `address`      varchar(255) NOT NULL COMMENT '地址',
  `create_time`  datetime     NOT NULL COMMENT '创建时间',
  `update_time`  datetime     NOT NULL COMMENT '更新时间'
) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8mb4 COMMENT='用户表';

CREATE TABLE ds_shop.`t_product` (
  `p_id`         bigint(20)   NOT NULL PRIMARY KEY AUTO_INCREMENT  COMMENT '主键，自增',
  `p_name`       varchar(255) NOT NULL COMMENT '产品名称',
  `p_code`       varchar(125) NOT NULL UNIQUE COMMENT '商品编号',
  `p_price`      decimal(7,2)  NOT NULL COMMENT '价格',
  `create_time`  datetime     NOT NULL COMMENT '创建时间',
  `update_time`  datetime     NOT NULL COMMENT '更新时间',
  `p_status`     tinyint(1)   NOT NULL DEFAULT 0 COMMENT '0 启用， 1 删除'  
) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8mb4 COMMENT='商品表';

CREATE TABLE ds_shop.`t_order` (
  `o_id`         char(36)     NOT NULL PRIMARY KEY   COMMENT '主键UUID',
  `p_code`       varchar(125) NOT NULL COMMENT '订单编号',
  `P_num`        int(11)      NOT NULL COMMENT '数量',
  `total_price`  decimal(7,2)   NOT NULL COMMENT '总价',
  `create_time`  bigint(20)     NOT NULL COMMENT '创建时间',
  `update_time`  bigint(20)     NOT NULL COMMENT '更新时间',
  `o_status`     tinyint(1)   NOT NULL DEFAULT 0 COMMENT '0 代付款， 1 已付款， 2 删除'  
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='订单表';
```

### 2.3 安装关系型数据库

3、（选做）：尽可能多的从“常见关系数据库”中列的清单，安装运行，并使用上一题的 SQL 测试简单的增删改查。

[在docker上安装mysql和postgresql，并进行简单操作](https://github.com/hefrankeleyn/JAVARebuild/blob/main/Week_06_SQL%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/2021-12-26-%E5%88%A9%E7%94%A8docker%E5%AE%89%E8%A3%85mysql%E5%92%8CpostgreSQL.md)。

### 2.4 测试100万订单数据

4、（选做）：基于上一题，尝试对各个数据库测试100万订单数据的增删改查性能。

（1）首先创建出100万的数据

本来用jdbc进行导入，发现根本不行，太慢了

```java
    public void createBatchInsertSql() {
        Random random = new Random();
        StringBuilder sb = new StringBuilder();
        String filePath = "/Users/lifei/Downloads/demo_sql/2021-12-27-order.sql";
        try (FileWriter fw = new FileWriter(filePath)) {
            for (int k = BATCH_INSERT_NUM / step; k > 0; k--) {
                sb.append("INSERT INTO t_order(o_id, p_code, p_num, total_price, create_time, update_time) values ");
                for (int i = 1; i <= step; i++) {
                    int code = random.nextInt(step);
                    int pNum = Math.max(code / 100, 1);
                    double price = Double.parseDouble(String.format("%.2f", Double.parseDouble(code / 100 + ".1" + code)));
                    sb.append(String.format("('%s', '%s', %d, %.2f, %d, %d),", UUID.randomUUID(), "p_" + code, pNum, price * pNum, System.currentTimeMillis(), System.currentTimeMillis()));
                }
                fw.write(sb.substring(0, sb.toString().length()-1)+";");
                fw.write("\n");
                sb.delete(0, sb.length());
            }
        } catch (IOException e) {
            e.printStackTrace();
        }
    }
```

（2）然后用SQL命令进行导入

> mysql -h127.0.0.1 -P3307 -uroot -Dds_shop -p <2021-12-27-order.sql

> psql -h 127.0.0.1 -p 5433 -d ds_shop -U postgres -f 2021-12-27-order.sql

感觉mysql的批量导入要快一些

### 2.5 测试mysql不同引擎

5、（选做）：尝试对 MySQL 不同引擎下测试100万订单数据的增删改查性能。

> 和上一个题目类似，跳过

### 2.6 测试导入导出的性能测试

6、（选做）：模拟1000万订单数据，测试不同方式下导入导出（数据备份还原） MySQL 的速度，包括 jdbc 程序处理和命令行处理。思考和实践，如何提升处理效率。

通过上面实践。导入批量的数据，还是用命令行效率高。

导入数据的时候，可以先将表的索引等约束去掉，数据导入后再把索引等约束添加上。

###  2.7 尝试使用不同的数据库连接池

7、（选做）：对 MySQL 配置不同的数据库连接池（DBCP、C3P0、Druid、Hikari）， 测试增删改查100万次，对比性能，生成报告。

[DBCP、C3P0、Druid 对比](https://cloud.tencent.com/developer/article/1368903)。

